peft_type: "LORA"
task_type: "CAUSAL_LM"

r: 8
lora_alpha: 16
lora_dropout: 0.25

target_modules:
- "v_proj"
- "gate_proj"
- "down_proj"

modules_to_save:
- "embed_tokens"
- "lm_head"

bias: "none"
fan_in_fan_out: false
use_rslora: true
use_dora: false

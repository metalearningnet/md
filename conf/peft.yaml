peft_type: "LORA"
task_type: "CAUSAL_LM"

r: 8
lora_alpha: 16
lora_dropout: 0.1

target_modules:
- "q_proj"
- "v_proj"
- "embed_tokens"
- "lm_head"

fan_in_fan_out: false
use_rslora: true
use_dora: false

bias: "none"
modules_to_save:
- "embed_tokens"
- "lm_head"

init_lora_weights: "gaussian"
inference_mode: false

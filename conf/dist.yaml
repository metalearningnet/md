# Distributed Training Strategy
strategy: "deepspeed"       # Options: 
                            # - 'ddp': PyTorch's DistributedDataParallel
                            # - 'fsdp': Fully Sharded Data Parallel
                            # - 'deepspeed': Supports ZeRO optimization

# Cluster Configuration
num_nodes: 1                # Machines in cluster
main_address: ""            # Leave empty for single-node
main_port: 1112             # Network port for communication

# DeepSpeed Configuration
deepspeed:
  zero_optimization:
    stage: 0                # ZeRO stages:
                            # - 0: No optimization
                            # - 1: Optimizer state partitioning
                            # - 2: Gradient partitioning
                            # - 3: Parameter partitioning
  
  activation_checkpointing:
    enable: true            # Recomputation for memory savings
    use_reentrant: true     # false=memory-optimized, true=stable

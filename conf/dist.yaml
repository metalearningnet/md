# Distributed training strategy.
# Options:
#   - "ddp": Use Distributed Data Parallel
#   - "deepspeed": Use DeepSpeed integration
#   - "fsdp": Use Fully Sharded Data Parallel
strategy: "deepspeed"

# Number of nodes used in distributed training.
# Set >1 for multi-node training.
num_nodes: 1

# IP address or hostname of the rank 0 (master) node.
# Leave empty for single-node training.
main_address: ""

# Communication port for distributed training.
main_port: 1112

deepspeed:
  optimizer:
    type: "AdamW"
    params:
      lr: 1e-4
      betas: [ 0.9, 0.95 ]
      eps: 1e-8
      weight_decay: 0.01

  zero_optimization:
    stage: 2
    contiguous_gradients: true

  activation_checkpointing:
    enable: true
    use_reentrant: false

  bf16:
    enabled: false

  fp16:
    enabled: false

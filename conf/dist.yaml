strategy: "deepspeed" # Options: 'ddp' | 'fsdp' | 'deepspeed'

num_nodes: 1 # Number of nodes used in distributed training

main_address: "" # Leave empty for single-node training

main_port: 1112 # Communication port for distributed training

deepspeed:
  optimizer:
    type: "AdamW"
    params:
      lr: 1e-4
      betas: [ 0.9, 0.95 ]
      eps: 1e-8
      weight_decay: 0.01

  zero_optimization:
    stage: 2
    contiguous_gradients: true

  activation_checkpointing:
    enable: true
    use_reentrant: false

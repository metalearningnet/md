# Distributed training strategy.
# Options:
#   - "ddp" for Distributed Data Parallel
#   - "deepspeed" for DeepSpeed integration
#   - "fsdp" for Fully Sharded Data Parallel
strategy: "deepspeed"

# Number of nodes used in distributed training.
# Set >1 for multi-node training.
num_nodes: 1

# IP address or hostname of the rank 0 (master) node.
# Leave empty for single-node training.
main_address: ""

# Communication port for distributed training.
main_port: 1112

deepspeed:
  # Micro-batch size per GPU
  train_micro_batch_size_per_gpu: 16

  # Number of steps to accumulate gradients before updating weights
  gradient_accumulation_steps: 4

  # Optimizer settings
  optimizer:
    type: "AdamW"
    params:
      lr: 1e-4
      betas: [ 0.9, 0.95 ]
      eps: 1e-8
      weight_decay: 0.01

  zero_optimization:
    stage: 2

    offload_optimizer:
      device: "cpu"
      # nvme_path: "/nvme"
      pin_memory: true

    offload_param:
      device: "cpu"
      # nvme_path: "/nvme"
      pin_memory: true

    overlap_comm: true
    contiguous_gradients: true

  activation_checkpointing:
    enable: true
    use_reentrant: false

  bf16:
    enabled: false

  fp16:
    enabled: false

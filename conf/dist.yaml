# Distributed training strategy.
# Options:
#   - "ddp" for Distributed Data Parallel
#   - "deepspeed" for DeepSpeed integration
#   - "fsdp" for Fully Sharded Data Parallel
strategy: "deepspeed"

# Number of nodes used in distributed training.
# Set >1 for multi-node training.
num_nodes: 1

# IP address or hostname of the rank 0 (master) node.
# Leave empty for single-node training.
main_address: ""

# Communication port for distributed training.
main_port: 1112

deepspeed:
  train_micro_batch_size_per_gpu: 8
  gradient_accumulation_steps: 4

  optimizer:
    type: "AdamW"
    params:
      lr: 1e-4
      betas: [ 0.9, 0.95 ]
      eps: 1e-8
      weight_decay: 0.01

  zero_optimization:
    stage: 2
    offload_optimizer:
      device: "cpu"
    offload_param:
      device: "cpu"

  activation_checkpointing:
    enable: true
    use_reentrant: false

  bf16:
    enabled: true

  fp16:
    enabled: false

  elastic_checkpoint: false
